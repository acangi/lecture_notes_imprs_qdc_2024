{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d0b671",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/acangi/lecture_notes_imprs_qdc_2024/blob/main/mini_book/docs/notebooks/perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "```{hint}\n",
    "Run the notebook in the colab environment.\n",
    "```\n",
    "\n",
    "(h1:perceptron)=\n",
    "\n",
    "# Perceptron\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "### Biological Neurons\n",
    "#### Basic Facts about Neurons\n",
    "A **neuron**, also known as a nerve cell, is the basic functional unit of the nervous system. Neurons are specialized cells that receive, process and transmit information through electrical and chemical signals.\n",
    "\n",
    "A neuron is made up of three main parts:\n",
    "\n",
    "* **Cell body (soma)**:\n",
    "Contains the nucleus and other organelles necessary for basic cell functions.\n",
    "\n",
    "* **Dendrites**:\n",
    "Tree-like projections that receive signals from other nerve cells and transmit them to the cell body.\n",
    "\n",
    "* **Axon**:\n",
    "A long, thin extension that conducts electrical signals away from the cell body to other nerve cells, muscles or glands. The end of the axon branches into axon terminals that release neurotransmitters to transmit signals.\n",
    "\n",
    "```{figure} ../images/Neurons_cerebral_cortex.png\n",
    "---\n",
    "width: 400px\n",
    "name: Neurons in the cerebral cortex\n",
    "align: center\n",
    "---\n",
    "Neurons in the cerebral cortex\n",
    "```\n",
    "#### Anatomy and Function of Neurons\n",
    "The function of a neuron can be divided into several steps:\n",
    "\n",
    "* **Reception of signals**: Dendrites receive chemical signals from neighboring neurons. These signals lead to changes in the membrane potential of the neuron.\n",
    "\n",
    "**Generation of an action potential**: When the membrane potential reaches a certain threshold, an action potential is triggered. This is a rapid change in the membrane potential that spreads along the axon.\n",
    "\n",
    "* **Transmission of the action potential**: The action potential travels along the axon to the axon terminals. This occurs through a series of depolarizations and repolarizations of the cell membrane.\n",
    "\n",
    "**Signal transmission**: At the axon terminal, the action potential leads to the release of neurotransmitters into the synaptic gap, the space between the axon terminal of the sending neuron and the dendrites of the receiving neuron. The neurotransmitters bind to receptors on the membrane of the receiving neuron and can trigger a new electrical signal there.\n",
    "\n",
    "This variability of information transmission is reflected in the weights of artificial neuronal networks.\n",
    "\n",
    "```{figure} ../images/Neuron_anatomy.png\n",
    "---\n",
    "width: 600px\n",
    "name: Anatomie eines Neurons\n",
    "align: center\n",
    "---\n",
    "Anatomy of a neuron\n",
    "```\n",
    "\n",
    "### Artificial Neurons\n",
    "\n",
    "#### History and Development of artificial neural networks\n",
    "* **Early ideas and inspirations (1940s)**:\n",
    "\n",
    "  * McCulloch and Pitts (1943): They developed the first mathematical model of a neuron, laying the foundation for neural network theory. Their model showed that a neuron can function as a simple binary switch.\n",
    "  \n",
    "```{figure} ../images/McCulloch-Pitts-cell.png\n",
    "---\n",
    "width: 600px\n",
    "name: McCulloch-Pitts Neuron\n",
    "align: center\n",
    "---\n",
    "McCulloch-Pitts neuron\n",
    "```\n",
    "\n",
    "* **Perzeptron Model (1950s and 1960s)**:\n",
    "  \n",
    "  * Frank Rosenblatt (1958): He invented the perceptron, a simple single-level neural network that could solve linear classification tasks. The perceptron was capable of learning by adapting its weights through a simple learning process.\n",
    "\n",
    "  * Criticism of the perceptron (1969): Marvin Minsky and Seymour Papert published the book \"Perceptrons\" in which they pointed out the limitations of the perceptron, particularly its inability to solve nonlinear problems such as the XOR problem. This led to a decline in interest in neural networks.\n",
    "\n",
    "```{figure} ../images/Perceptron-xor-task.png\n",
    "---\n",
    "width: 600px\n",
    "name: Perzeptron-Modell\n",
    "align: center\n",
    "---\n",
    "Perzeptron model\n",
    "```\n",
    "* **Revival and development of the multilayer perceptron (1980s):**\n",
    "\n",
    "  * Backpropagation algorithm (1986): David Rumelhart, Geoffrey Hinton and Ronald Williams introduced the backpropagation algorithm to efficiently adjust the weights in multilayer neural networks. This solved the problem of non-linear classification and opened up new possibilities for artificial neural networks.\n",
    "\n",
    "## Perzeptron Model\n",
    "\n",
    "### Structure\n",
    "The simple perceptron model consists of an input layer and an output layer.\n",
    "\n",
    "In the input layer, the input is $\\vec{x} = (x_1, x_2,\\dots, x_n)$.\n",
    "\n",
    "The output layer consists of a single neuron. It contains the network input $z$ and the output value $y$.\n",
    "\n",
    "This network can be used for binary classification, i.e. the network can decide for an input whether it belongs to a certain category. The classification is expressed by the output value $y$.\n",
    "\n",
    "```{figure} ../images/Perceptron.png\n",
    "---\n",
    "width: 600px\n",
    "name: Perceptron\n",
    "align: center\n",
    "---\n",
    "Perceptron\n",
    "```\n",
    "\n",
    "### Forward Propagation\n",
    "In the following, the functionality of the simple perceptron model will be demonstrated with the help of vector notation and the path from input $\\vec{x}$ to output $y$ will be traced. These steps are also referred to as forward propagation.\n",
    "\n",
    "The input is represented as a feature vector $\\vec{x} = (x_1, x_2,\\dots, x_n)$. In other words\n",
    "\\begin{align}\n",
    "\\vec{x} &=\n",
    "\\begin{pmatrix}\n",
    "x_1\\\\ \\vdots\\\\ x_n\n",
    "\\end{pmatrix} ,\n",
    "\\end{align}\n",
    "where the feature vector has length $n$ und a feature is denoted by $x_i \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "The parameters of the perceptron are the weights $w_i \\in \\mathbb{R}$. They are represented in terms of a so-called weight vector:\n",
    "\\begin{align}\n",
    "\\vec{w} &=\n",
    "\\begin{pmatrix}\n",
    "w_1\\\\ \\vdots\\\\ w_n\n",
    "\\end{pmatrix} .\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "#### Network Input\n",
    "In the first step, the network input $z$ is calculated from the feature vector $\\vec{x}$. This results from the sum of the values of the input neurons multiplied by their respective weights:\n",
    "\\begin{align}\n",
    "z &= \\sum_{i=1}^n w_i x_i = w_1 x_1 + \\dots + w_n x_n\\ . \n",
    "\\end{align}\n",
    "\n",
    "We can express this in a compact form in vector notation as matrix multiplication:\n",
    "\\begin{align}\n",
    "z &= \\vec{w}^T \\vec{x}\\\\\n",
    "  &= (w_1, \\dots, w_n) \\begin{pmatrix} x_1\\\\ \\vdots\\\\ x_n \\end{pmatrix}\\\\\n",
    "  &= w_1 x_1 + \\dots + w_n x_n\\,,  \n",
    "\\end{align}\n",
    "where $\\vec{w}^T$ denotes a transposed weight vector (which is a row vector).\n",
    "\n",
    "#### Activation and Output\n",
    "In the second and final step, we calculate the activation of the output neuron, which also corresponds to the output of the perceptron model.\n",
    "\n",
    "An activation function $g$ is applied to the network input:\n",
    "\\begin{align}\n",
    "y &= g(z)\\ .\n",
    "\\end{align}\n",
    "\n",
    "### Learning Process\n",
    "Now that we have understood the propagation of data through the perceptron, let us turn our attention to the learning process. In the context of the perceptron model, we understand learning as the gradual adaptation of the weights $\\vec{w}$ to the desired target function with the help of **training data**.\n",
    "\n",
    "#### Training Data\n",
    "Training data is, for example, a series of data with a label (e.g. images with the categorical assignment \"cat\" and \"non-cat\").  The training data can therefore be written as pairs of feature vectors and labels:\n",
    "\\begin{align}\n",
    "(\\vec{x}^k, y^k) ~~~ k \\in \\{1, \\dots, N\\}\\,,\n",
    "\\end{align}\n",
    "where we assume that the training data has a dimension $N$.\n",
    "\n",
    "With a neural network, we must distinguish between the calculated output of the current network $\\bar{y}^k \\in \\{0,1\\}$ and the correct output of a training example $y^k \\in \\{0,1\\}$.\n",
    "\n",
    "\n",
    "#### Learning Step\n",
    "Learning means that we calculate the output $\\bar{y}$ for each training example and then adjust the weights. This is called a **learning step**.\n",
    "\n",
    "We can therefore adjust the weight vector $\\vec{w}$ for the pairs of feature vectors and labels $(\\vec{x}^k, y^k)$ in each learning step by adding a change of all weights to the current value:\n",
    "\\begin{align}\n",
    "w_i := w_i + \\Delta w_i\\ .\n",
    "\\end{align}\n",
    "\n",
    "We will define a concrete rule for adjusting the weights in the next section. Before that, however, let's look at the properties of the weight update:\n",
    "\n",
    "* If the calculated output $\\bar{y}$ is greater than the reference value $y$, the weight update should be negative, i.e. the weight of this neuron should be weakened.\n",
    "* If the calculated output $\\bar{y}$ is smaller than the reference value $y$, the weight update should be positive, i.e. the weight is increased.\n",
    "* If the calculated output $\\bar{y}$ and the reference value $y$ are the same, no weighting update should take place.\n",
    "\n",
    "### Gradient Descent\n",
    "In the following, we will derive a learning rule that can be used to update the weights of a perceptron.\n",
    "\n",
    "#### Loss Function\n",
    "In order to derive a suitable learning rule, we must first define the term loss function $L$. It is defined as\n",
    "\\begin{align}\n",
    "L(w) &= \\frac{1}{2N} \\sum_{k=1}^N \\left[ y^k - \\bar{y}^k \\right]^2\\ . \n",
    "\\end{align}\n",
    "\n",
    "The loss function is therefore defined as the mean value of the squared errors, which is very common.\n",
    "\n",
    "How can you visualize the loss function? It is a multidimensional function of the weights or the weight vector $\\vec{w}$. An intuitive idea can be obtained by looking at the figure in which the loss function is represented as a function of two representative weights ($w_0$ and $w_1$).\n",
    "\n",
    "```{figure} ../images/landscape_0.png\n",
    "---\n",
    "width: 600px\n",
    "name: landscape_0\n",
    "align: center\n",
    "---\n",
    "Parameter landscape of the loss function\n",
    "```\n",
    "\n",
    "We can see that the loss function here spans a two-dimensional (and generally a multi-dimensional) parameter landscape.\n",
    "\n",
    "#### Learning rule: Minimum of the loss landscape\n",
    "We can now use the loss function to derive a learning rule by setting ourselves the goal of minimizing the value of the loss function. This means that we look for valleys in the parameter landscape.\n",
    "\n",
    "```{figure} ../images/landscape_2.png\n",
    "---\n",
    "width: 600px\n",
    "name: landscape_2\n",
    "align: center\n",
    "---\n",
    "Gradient descent in the parameter landscape of the loss function\n",
    "```\n",
    "### Weight Update\n",
    "With this goal in mind, we can derive the weight update $w := w + \\Delta w$. To do this, we use the gradient of the loss landscape, which informs us about the largest increase in the error landscape. We therefore take the negative gradient and thus obtain our learning rule:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta \\vec{w} &= -\\alpha\\, \\nabla L(w)\\\\\n",
    "         &= -\\alpha\\, \\begin{pmatrix} \\frac{\\partial L(w)}{\\partial w_1}\\\\\n",
    "                            \\vdots \\\\\n",
    "                            \\frac{\\partial L(w)}{\\partial w_n}\n",
    "            \\end{pmatrix}\\,,\n",
    "\\end{align}\n",
    "where we have also introduced the learning rate $\\alpha \\in [0,1]$.\n",
    "As can be seen in the figure, updating the weights in the direction of the negative gradient causes us to run in the direction of the minimum of the loss landscape.\n",
    "\n",
    "For the learning rule, we now need the partial derivatives of the loss function according to the weights $w_1, \\dots, w_n$, which we now calculate.\n",
    "\n",
    "To do this, we start with the definition of the loss function and reshape it until we can derive it according to the individual weights:\n",
    "\n",
    "\\begin{align}\n",
    "L(w) &= \\frac{1}{2N} \\sum_{k=1}^N \\left[ y^k - \\bar{y}^k \\right]^2\\ .\n",
    "\\end{align}\n",
    "\n",
    "To simplify the derivation, we choose the identical mapping $g(z) = z$ as the activation function.\n",
    "\n",
    "We insert it into the definition of the loss function and obtain\n",
    "\\begin{align}\n",
    "L(w) &= \\frac{1}{2N} \\sum_{k=1}^N \\left[ y^k - g(z^k) \\right]^2\\ .\n",
    "\\end{align}\n",
    "\n",
    "We now evaluate the activation function, which simplifies the expression to:\n",
    "\\begin{align}\n",
    "L(w) &= \\frac{1}{2N} \\sum_{k=1}^N \\left( y^k - z^k \\right)^2\\ .\n",
    "\\end{align}\n",
    "\n",
    "Now we insert the expression for the network input:\n",
    "\\begin{align}\n",
    "L(w) &= \\frac{1}{2N} \\sum_{k=1}^N \\left( y^k - \\vec{w}^T \\vec{x}^k \\right)^2\\ .\n",
    "\\end{align}\n",
    "\n",
    "Now we can explicitly calculate the derivative with respect to the weight $w_i$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_i}\n",
    "&= \\frac{\\partial}{\\partial w_i} \\frac{1}{2N} \\sum_{k=1}^N \\left( y^k - \\vec{w}^T \\vec{x}^k \\right)^2 \\\\\n",
    "&= \\frac{1}{2N} \\sum_{k=1}^N \\frac{\\partial}{\\partial w_i} \\left( y^k - \\vec{w}^T \\vec{x}^k \\right)^2 \\\\\n",
    "&= \\frac{1}{2N} \\sum_{k=1}^N  2 \\left( y^k - \\vec{w}^T \\vec{x}^k \\right) \\frac{\\partial}{\\partial w_i} \\left(y^k - \\vec{w}^T \\vec{x}^k \\right) \\\\\n",
    "&= \\frac{1}{N} \\sum_{k=1}^N \\left( y^k - \\vec{w}^T \\vec{x}^k \\right) \\frac{\\partial}{\\partial w_i} \\left[-(w_1, \\dots, w_i, \\dots, w_n) \\begin{pmatrix} x^k_1\\\\ \\vdots\\\\  x^k_i\\\\ \\vdots\\\\ x^k_n \\end{pmatrix} \\right] \\\\\n",
    "&= -\\frac{1}{N} \\sum_{k=1}^N \\left( y^k - \\vec{w}^T \\vec{x}^k \\right) \\frac{\\partial}{\\partial w_i} \\left[ (w_1 x^k_1 + \\dots + w_i x^k_i + \\dots + w_n x^k_n) \\right] \\\\\n",
    "&= -\\frac{1}{N} \\sum_{k=1}^N \\left( y^k - \\vec{w}^T \\vec{x}^k \\right) x^k_i\\ .\n",
    "\\end{align}\n",
    "\n",
    "At the end we use the definition of the activation function again and obtain:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial w_i}\n",
    "&=-\\frac{1}{N} \\sum_{k=1}^N \\left[ y^k - g(z^k) \\right] x^k_i\\ .\n",
    "\\end{align}\n",
    "\n",
    "This gives us a learning rule for a specific weight:\n",
    "\\begin{align}\n",
    "\\Delta w_i\n",
    "&=-\\alpha \\frac{\\partial L(w)}{\\partial w_i}\\\\\n",
    "&=\\frac{\\alpha}{N} \\sum_{k=1}^N \\left[ y^k - g(z^k) \\right] x^k_i\\ .\n",
    "\\end{align}\n",
    "\n",
    "Since the output of the perceptron is $\\bar{y}^k=g(z^k)$, we thus obtain\n",
    "\\begin{align}\n",
    "\\Delta w_i\n",
    "&=\\frac{\\alpha}{N} \\sum_{k=1}^N \\left( y^k - \\bar{y}^k \\right) x^k_i\\ .\n",
    "\\end{align}\n",
    "\n",
    "Thus, we have derived our learning rule using gradient descent. As we can see, we first need to process the sum over all training data ($N$ times) to calculate our weight update for a learning step.\n",
    "\n",
    "# Application example: Binary Classification with the Percepetron Model\n",
    "\n",
    "### Learning Algorithm\n",
    "With the help of the derived learning rule, we can now formulate a learning algorithm.\n",
    "\n",
    "1. initialize all weights $\\vec{w} = (w_0, ..., w_n)$.\n",
    "2. for each epoch:\n",
    "     * Set $\\Delta w_i = 0$\n",
    "     * For each set of training data ($x^k, y^k$), $k=1, \\dots, N$:\n",
    "       * Calculate output $y^k$.\n",
    "       * Calculate weight update: $\\Delta w_i^k = \\Delta w_i^k + (y^k-\\bar{y}^k)x_i^k$.\n",
    "     * Calculate the mean of all weight updates over the training data: $\\Delta w_i = \\frac{\\alpha}{N}\\sum_{k=1}^N \\Delta w_i^k$.\n",
    "     * Update all weights $w_i = w_i + \\Delta w_i$\n",
    "\n",
    "\n",
    "### Implementation of the Perceptron\n",
    "#### Perceptron Class\n",
    "First we define a class `Perceptron`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron():\n",
    "    \n",
    "    # Constructor with default values\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    # Training with x (features) and y (labels)\n",
    "    def train(self, x, y, epochs):\n",
    "        rnd = np.random.RandomState(42)\n",
    "        n_samples = x.shape[0]\n",
    "        print(f\"Train on {n_samples} samples\")\n",
    "        \n",
    "        # Weights: 1 + dim(x) for the bias neuron\n",
    "        self.w = rnd.normal(loc=0, scale=0.01, size=1 + x.shape[1]) \n",
    "    \n",
    "        # List for storing loss function in each epoch\n",
    "        self.loss = [] \n",
    "        \n",
    "        # List for storing prediction in each epoch\n",
    "        self.prediction = [] \n",
    "\n",
    "        # List for storing binary accuracy in each epoch\n",
    "        self.accuracy = [] \n",
    "\n",
    "        # Loop over epochs\n",
    "        for i in range(epochs):\n",
    "            z = self.net_input(x) # Network input for all training data\n",
    "            y_hat = self.activation(z) # Activation for all training data\n",
    "            diff = y - y_hat # Loss vector for all training data\n",
    "            \n",
    "            # Weight update\n",
    "            self.w[1:] += self.alpha * x.T.dot(diff)\n",
    "            \n",
    "            # Weight update for bias neuron\n",
    "            self.w[0] += self.alpha * diff.sum()\n",
    "            \n",
    "            # Save SSE of loss for each epoch\n",
    "            l = (diff**2).sum() / 2.0 \n",
    "            self.loss.append(l)\n",
    "            print(f\"Epoch {i+1}/{epochs} - loss: {l:.4f}\")\n",
    "\n",
    "            # Save prediction for each epoch\n",
    "            prediction = self.predict(x)\n",
    "            self.prediction.append(prediction)\n",
    "\n",
    "            # Save binary accuracy for each epoch\n",
    "            accuracy = self.measure_accuracy(y,self.predict(x))\n",
    "            self.accuracy.append(accuracy)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    # Activation function\n",
    "    def activation(self, z):\n",
    "        return z\n",
    "    \n",
    "    def net_input(self, x):\n",
    "        return np.dot(x, self.w[1:]) + self.w[0] \n",
    "    \n",
    "    def predict(self, x):\n",
    "        return np.where(self.net_input(x) >= 0, 1, 0)\n",
    "\n",
    "    def measure_accuracy(self, y_true, y_pred):\n",
    "        N = y_true.shape[0]\n",
    "        accuracy = (y_true == y_pred).sum() / N\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc437e",
   "metadata": {},
   "source": [
    "#### Learning Process\n",
    "The learning or training of the perceptron takes place in the function 'train', in particular in the loop over the epochs (lines 20 - 35).\n",
    "\n",
    "```\n",
    "for i in range(epochs):\n",
    "            z = self.net_input(x) # Network input for all training data\n",
    "            y_hat = self.activation(z) # Activation for all training data\n",
    "            diff = y - y_hat # Loss vector for all training data\n",
    "            \n",
    "            # Weight update\n",
    "            self.w[1:] += self.alpha * x.T.dot(diff)\n",
    "            \n",
    "            # Weight update for bias neuron\n",
    "            self.w[0] += self.alpha * diff.sum()\n",
    "```\n",
    "\n",
    "\n",
    "#### Batch processing of the training data\n",
    "We have implemented the perceptron in such a way that the matrix $x$ contains all feature vectors. This ensures that all training data is run through before the weights are updated. \n",
    "The feature vectors are stacked on top of each other to form an $N \\times 3$ input matrix. It should be noted that the first column of the matrix represents the bias neuron, i.e. contains only ones. By matrix multiplying the $N \\times 3$ input matrix with the $3 \\times 1$ weighting vector, we obtain an $N \\times 1$ output vector that contains all outputs for all training examples.\n",
    "\n",
    "The net input is calculated in the `net_input` function:\n",
    "```\n",
    "def net_input(self, x):\n",
    "        return np.dot(x, self.w[1:]) + self.w[0] \n",
    "```\n",
    "\n",
    "To give us a better understanding, we illustrate the operations.\n",
    "\n",
    "```{figure} ../images/Training_01.png\n",
    "---\n",
    "width: 600px\n",
    "name: training_01\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "The input vectors are actually row vectors and stacked on top of each other.\n",
    "\n",
    "However, this picture is not entirely correct. In fact, the bias neuron is treated differently in the implementation, as the following figure shows:\n",
    "\n",
    "```{figure} ../images/Training_02.png\n",
    "---\n",
    "width: 600px\n",
    "name: training_02\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "This corresponds to the following line in the implementation:\n",
    "```\n",
    "np.dot(x, self.w[1:]) + self.w[0]\n",
    "```\n",
    "\n",
    "#### Weight Update \n",
    "The weight update\n",
    "\\begin{align}\n",
    "\\vec{w} &= \\vec{w} + \\Delta\\vec{w}\\\\\n",
    "\\Delta w_i &= -\\alpha\\frac{\\partial L}{\\partial w_i} = \\frac{\\alpha}{N}\\sum_{k=1}^N \\left[y^k - g(z^k) \\right] x_i^k\n",
    "\\end{align}\n",
    "is implemented in the following way.\n",
    "\n",
    "```{figure} ../images/Training_03.png\n",
    "---\n",
    "width: 600px\n",
    "name: training_03\n",
    "align: center\n",
    "---\n",
    "```\n",
    "\n",
    "### Data Set\n",
    "The sample data sets of the `scikit-learn` library can be used to illustrate the perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648538d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import LinearLocator\n",
    "from matplotlib import cbook\n",
    "from matplotlib.colors import LightSource\n",
    "from mpl_toolkits.mplot3d import axes3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dac06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data_wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403186be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wine.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61858ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_wine.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922917d",
   "metadata": {},
   "source": [
    "The output data is saved in the `target` category. The input data is located in the `data` category. The following features are available as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34368493",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wine.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3485d23",
   "metadata": {},
   "source": [
    "In our example, we want to use two features to solve a binary classification problem using the perceptron model. Therefore, we analyze the dataset by plotting the three categories as a function of different features. It turns out that the two features \"alcohol\" and \"flavonoids\" lead to a linearly separable data set between the categories \"0\" (dark blue) and \"2\" (yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e87a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_wine.data.T[0], data_wine.data.T[6], c=data_wine.target)\n",
    "plt.xlabel(data_wine.feature_names[0])\n",
    "plt.ylabel(data_wine.feature_names[6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe02a5c",
   "metadata": {},
   "source": [
    "We now prepare our data set for training the perceptron model. We start with the output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb6958",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate((data_wine.target[:59], data_wine.target[130:]))\n",
    "# Change values to 0 and 1.\n",
    "y = np.where(y == 2, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c01e5",
   "metadata": {},
   "source": [
    "Similarly, we prepare the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29186d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = data_wine.data.T\n",
    "input_features = input_features[[0,6]]\n",
    "x = np.concatenate((input_features.T[:59], input_features.T[130:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a66650b",
   "metadata": {},
   "source": [
    "At the end, we check once again whether the data has been prepared correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dcda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x.T[0][:59], x.T[1][:59], c='blue', label='Kategorie 0')\n",
    "plt.scatter(x.T[0][59:], x.T[1][59:], c='red', label='Kategorie 1')\n",
    "plt.xlabel(data_wine.feature_names[0])\n",
    "plt.ylabel(data_wine.feature_names[6])\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e620b42",
   "metadata": {},
   "source": [
    "### Training Examples\n",
    "\n",
    "We train a model with a learning rate of $\\alpha=0.01$ over a period of 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd3f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Perceptron(alpha=0.01)\n",
    "model1.train(x, y, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505b23e",
   "metadata": {},
   "source": [
    "We then train another model with a learning rate of $\\alpha=0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494dd125",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Perceptron(alpha=0.0001)\n",
    "model2.train(x, y, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b282ad",
   "metadata": {},
   "source": [
    "We illustrate the loss function (top) and the accuracy (bottom) for training with both learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e637a8",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16,12))\n",
    "\n",
    "ax[0][0].plot(range(1, len(model1.loss)+1), np.log10(model1.loss), marker='o')\n",
    "ax[0][0].set_xlabel('Epochen')\n",
    "ax[0][0].set_ylabel('log(SSE)')\n",
    "ax[0][0].set_title('Perzeptron, alpha=0.01')\n",
    "\n",
    "ax[0][1].plot(range(1, len(model2.loss)+1), np.log10(model2.loss), marker='o')\n",
    "ax[0][1].set_xlabel('Epochen')\n",
    "ax[0][1].set_ylabel('log(SSE)')\n",
    "ax[0][1].set_title('Perzeptron, alpha=0.0001')\n",
    "\n",
    "ax[1][0].plot(range(1, len(model1.loss)+1), model1.accuracy, marker='o')\n",
    "ax[1][0].set_ylim(0, 1)\n",
    "ax[1][0].set_xlabel('Epochen')\n",
    "ax[1][0].set_ylabel('Accuracy')\n",
    "ax[1][0].set_title('Perzeptron, alpha=0.01')\n",
    "\n",
    "ax[1][1].plot(range(1, len(model2.loss)+1), model2.accuracy, marker='o')\n",
    "ax[1][1].set_ylim(0, 1)\n",
    "ax[1][1].set_xlabel('Epochen')\n",
    "ax[1][1].set_ylabel('Accuracy')\n",
    "ax[1][1].set_title('Perzeptron, alpha=0.0001')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3aee7",
   "metadata": {},
   "source": [
    "## Training strategies\n",
    "\n",
    "### Standardization\n",
    "Standardize the two inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae05cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_st = np.copy(x)\n",
    "x_st[:, 0] = (x[:,0] - x[:,0].mean()) / x[:,0].std()\n",
    "x_st[:, 1] = (x[:,1] - x[:,1].mean()) / x[:,1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f45e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Perceptron(alpha=0.01)\n",
    "model3.train(x_st, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79de035",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(16,12))\n",
    "\n",
    "ax[0][0].plot(range(1, len(model1.loss)+1), np.log10(model1.loss), marker='o')\n",
    "ax[0][0].set_xlabel('Epochen')\n",
    "ax[0][0].set_ylabel('log(SSE)')\n",
    "ax[0][0].set_title('Perzeptron, alpha=0.01')\n",
    "\n",
    "ax[0][1].plot(range(1, len(model2.loss)+1), np.log10(model2.loss), marker='o')\n",
    "ax[0][1].set_xlabel('Epochen')\n",
    "ax[0][1].set_ylabel('log(SSE)')\n",
    "ax[0][1].set_title('Perzeptron, alpha=0.001')\n",
    "\n",
    "ax[0][2].plot(range(1, len(model3.loss)+1), np.log10(model3.loss), marker='o')\n",
    "ax[0][2].set_xlabel('Epochen')\n",
    "ax[0][2].set_ylabel('log(SSE)')\n",
    "ax[0][2].set_title('Perzeptron, std, alpha=0.01')\n",
    "\n",
    "ax[1][0].plot(range(1, len(model1.loss)+1), model1.accuracy, marker='o')\n",
    "ax[1][0].set_ylim(0, 1)\n",
    "ax[1][0].set_xlabel('Epochen')\n",
    "ax[1][0].set_ylabel('Accuracy')\n",
    "ax[1][0].set_title('Perzeptron, alpha=0.01')\n",
    "\n",
    "ax[1][1].plot(range(1, len(model2.loss)+1), model2.accuracy, marker='o')\n",
    "ax[1][1].set_ylim(0, 1)\n",
    "ax[1][1].set_xlabel('Epochen')\n",
    "ax[1][1].set_ylabel('Accuracy')\n",
    "ax[1][1].set_title('Perzeptron, alpha=0.001')\n",
    "\n",
    "ax[1][2].plot(range(1, len(model3.loss)+1), model3.accuracy, marker='o')\n",
    "ax[1][2].set_ylim(0, 1)\n",
    "ax[1][2].set_xlabel('Epochen')\n",
    "ax[1][2].set_ylabel('Accuracy')\n",
    "ax[1][2].set_title('Perzeptron, std, alpha=0.01')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "mystnb": {
   "execution_mode": "inline"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
